# -*- coding: utf-8 -*-
"""Roidouli_CaseStudy1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hKEnPiTCeIDEZOxzE_Ek-3Kmrx1uwVdA

Author: **Roidouli Vasiliana**

# **Dataset Loading and Libraries Importing**
"""

# Libraries Importing
import pandas as pd

from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import datetime

from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.decomposition import PCA
from sklearn.feature_selection import RFE, mutual_info_regression

from keras.callbacks import EarlyStopping

import warnings
warnings.filterwarnings("ignore")

# Display all columns and rows of the dataset
pd.options.display.max_columns = None
pd.options.display.max_rows = None

drive.mount('/content/gdrive')

"""In order to reproduce the results of this Code, it is needed to update the path with the path that your file exists."""

# Load dataset
df = pd.read_csv("/content/gdrive/MyDrive/stout/loans_full_schema.csv")

"""# **Part A: Analysis of the Dataset and Visualizations**

## **Quick Look at the Dataset**

The following dataset has **10.000 observiations** with **55 variables** and it represents thousands of loans made through the Lending Club platform, which is a peer-to-peer lending platform that allows individuals to borrow and lend money to each other without going through traditional banks.
"""

# Size of dataset
df.shape

# Show 5 random rows of dataset
df.sample(5)

# Quick look in the columns - empty values - datatypes
df.info()

"""## **Analysis of Categorical and Numerical Variables**

#### **A. Categorical Variables**

We have 13 **categorical** variables: 
* `emp_title`: job title of the applicant. (manager, owner, teacher etc)
* `state`: two-letter state code. (CA, TX, NY etc)
* `homeownership`: the ownership status of the applicant's residence. (MORTGAGE, RENT, OWN)
* `verified_income`: type of verification of the applicant's income. (Source Verified, Not Verified, Verified)
* `verification_income_joint`: type of verification of the joint income (Source Verified, Not Verified, Verified)
* `loan_purpose`: category for the purpose of the loan. (debt consolidation, credit card, home improvement etc)
* `application_type`: type of application (individual, joint)
* `grade`: grade associated with the loan. The grade reflects the creditworthiness and the risk associated with the loan. In grading system "A-D", "A" referes to the highest quality and "D" to the lowest. (A, B, C, D, E, F, G)
* `sub_grade`: detailed grade associated with the loan (A1, A2, B1, B2, etc)
* `issue_month`: month the loan was issued (Jan-2018, Feb-2018, Mar-2018)
* `loan_status`: status of the loan (Current, Fully Paid, In Grace Period, Late (31-120 days), Late (16-30 days))
* `initial_listing_status`: initial listing status of the loan (whole, fractional)
* `disbursement_method`: dispersement method of the loan. (Cash, DirectPay)
"""

# Categorical values
df.describe(include='object')

"""#### **B. Numerical Variables**

We have 42 **numerical** variables: 
* `emp_length`: Number of years in the job, rounded down. 
* `annual_income`: The annual income of the applicant.
* `debt_to_income`: Debt-to-income ratio.  
* `annual_income_joint`: if this is a joint application, then the annual income of the two parties applying.
* `debt_to_income_joint`: Debt-to-income ratio for the two parties.
* `delinq_2y`: Delinquencies on lines of credit in the last 2 years. 
* `months_since_last_delinq`: Months since the last delinquency.
* `earliest_credit_line`: Year of the applicant's earliest line of credit. 
* `inquiries_last_12m`: Inquiries into the applicant's credit during the last 12 months. 
* `total_credit_lines`: Total number of credit lines in this applicant's credit history. 
* `open_credit_lines`: Number of currently open lines of credit. 
* `total_credit_limit`: Total available credit, e.g. if only credit cards, then the total of all the credit limits. This excludes a mortgage.
* `total_credit_utilized`: Total credit balance, excluding a mortgage.
* `num_collections_last_12m`: Number of collections in the last 12 months. This excludes medical collections. 
* `num_historical_failed_to_pay`: The number of derogatory public records, which roughly means the number of times the applicant failed to pay.
* `months_since_90d_late`: Months since the last time the applicant was 90 days late on a payment.
* `current_accounts_delinq`: Number of accounts where the applicant is currently delinquent. 
* `total_collection_amount_ever`: The total amount that the applicant has had against them in collections. 
* `current_installment_accounts`: Number of installment accounts, which are (roughly) accounts with a fixed payment amount and period. A typical example might be a 36-month car loan. 
* `accounts_opened_24m`: Number of new lines of credit opened in the last 24 months
* `months_since_last_credit_inquiry`:  Number of months since the last credit inquiry on this applicant. 
* `num_satisfactory_accounts`: Number of satisfactory accounts. 
* `num_accounts_120d_past_due`: Number of current accounts that are 120 days past due. 
* `num_accounts_30d_past_due`: Number of current accounts that are 30 days past due. 
* `num_active_debit_accounts`: Number of currently active bank cards. 
* `total_debit_limit`: Total of all bank card limits.
* `num_total_cc_accounts`: Total number of credit card accounts in the applicant's history. 
* `num_open_cc_accounts`: Total number of currently open credit card accounts.
* `num_cc_carrying_balance`: Number of credit cards that are carrying a balance.
* `num_mort_accounts`: Number of mortgage accounts.
* `account_never_delinq_percent`: Percent of all lines of credit where the applicant was never delinquent.
* `tax_liens`: a numeric vector 
* `public_record_bankrupt`: Number of bankruptcies listed in the public record for this applicant. 
* `loan_amount`: The amount of the loan the applicant received. 
* `term`: The number of months of the loan the applicant received.
* `interest_rate`: Interest rate of the loan the applicant received.
* `installment`: Monthly payment for the loan the applicant received. 
* `balance`: Current balance on the loan. 
* `paid_total`: Total that has been paid on the loan by the applicant.
* `paid_principal`: The difference between the original loan amount and the current balance on the loan.
* `paid_interest`: The amount of interest paid so far by the applicant.
* `paid_late_fees`: Late fees paid by the applicant.
"""

# Numerical values
df.describe()

"""## **Null Values Handling**"""

# Check if there are any empty cells in the dataset
df.isnull().values.any()

def plot_null_values(df):
  if df.isnull().sum().sum() != 0:
    na_df = (df.isnull().sum() / len(df)) * 100
    na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)
    missing_data = pd.DataFrame({'Missing Ratio %' :na_df})
    missing_data.plot(kind = "barh")
    plt.show()
  else:
    print('No NAs found')
  return

plot_null_values(df)

"""Null values add bias into the analysis if they are not handled properly and also can affect the performance of a machine learning model. Many machine learning algorithms cannot handle null values and will fail or produce inaccurate results. Removing null values can help ensure that the machine learning model is trained on a clean, consistent dataset. """

for column in df.columns:
  if df[column].isna().sum() != 0 :
    missing = df[column].isna().sum()
    portion = (missing / df.shape[0]) * 100
    print(f"'{column}': number of missing values '{missing}' ==> '{portion:.3f}%'")

"""* `emp_title` : """

df.emp_title.nunique()

"""Realistically there are **too many unique** job titles to try to convert this to a dummy variable feature. We will remove the `emp_title` column."""

df.drop(columns=['emp_title'], inplace=True)

"""* `emp_length` : """

df.emp_length.unique()

for year in df.emp_length.unique():
    print(f"{year} years in this position:")
    print(f"{df[df.emp_length == year].loan_status.value_counts(normalize=True)}")
    print('==========================================')

"""Rates are extremely similar across all employment lengths. So we are going to drop the `emp_length` column."""

df.drop(columns=['emp_length'], inplace=True)

"""* `debt_to_income` :"""

df.debt_to_income.value_counts().head()

"""The null value ration of `debt_to_income` is very small, so we replace the null values with the mean value of this column."""

df['debt_to_income'].fillna(df['debt_to_income'].mean(), inplace=True)

"""* `annual_income_joint` , `debt_to_income_joint`, `verification_income_joint`

The fact that there are too many null values refer to no joint. We are going to drop these columns.
"""

df.drop(columns=['annual_income_joint', 'debt_to_income_joint', 'verification_income_joint'], inplace=True)

"""* `months_since_last_delinq` : """

df.months_since_last_delinq.value_counts().head()

"""The null value ratio of `months_since_last_delinq` is somehow large. The null values refer to the fact the there was never an delinquency. So we could set a very big number or a number bigger than the maximum value of this column, so as to not confuse the model by setting the zero value. """

df['months_since_last_delinq'].fillna(df['months_since_last_delinq'].max() + 24, inplace=True)

"""* `months_since_90d_late` :"""

df.months_since_90d_late.value_counts().head(10)

"""There are too many null values, so we are going to drop the `months_since_90d_late` column."""

df.drop(columns=['months_since_90d_late'], inplace=True)

"""* `months_since_last_credit_inquiry` :"""

df.months_since_last_credit_inquiry.value_counts().head(10)

"""The null value ratio of `months_since_last_credit_inquiry` is **not large**. It wouldn't be a good idea to drop the column. The null values refer to the fact the there was never an inquiry. So we could set a very big number or a number bigger than the maximum value of this column, so as to not confuse the model by setting the zero value. """

df['months_since_last_credit_inquiry'].fillna(df['months_since_last_credit_inquiry'].max() + 24, inplace=True)

"""* `num_accounts_120d_past_due` : """

df.num_accounts_120d_past_due.value_counts().head()

"""All values are zero values, so we will drop the `num_accounts_120d_past_due` column."""

df.drop(columns=['num_accounts_120d_past_due'], inplace=True)

"""Let's check for null values again"""

df.isnull().values.any()

"""## **Visualizations**

The Visualizations can give us an understanding for which variables are important.

* `Bar Chart Showing the distribution of Loan Grades (A-G)`

The bar chart showing the distribution of Loan Grades (A-G) showed that there are more loans in grade B than in grade A, more loans in grade C than in grade B, and so on down to grade G, this could potentially indicate that the Lending Club is more likely to approve loans for borrowers with higher credit scores.  It could also suggest that borrowers with lower credit scores are less likely to use the platform, or that the Lending Club is more selective in the loans that it approves for these borrowers.
"""

plt.figure(figsize=(20, 7))
loan_grades = df.grade.unique().tolist()
plt.subplot(2, 2, 1)
sns.countplot(y='grade', data=df)
plt.title('Distribution of Loan Grades')
plt.xlabel('Loan Grades')
plt.ylabel('Frequency')
plt.show()

"""* `Histogram of the distribution of loan amounts`

The histogram shows the frequency of different loan amounts. The histogram uses a set of bins to group the loan amounts into intervals, and the height of each bar represents the frequency of loans with loan amounts within that interval. We can see that the majority of loans have loan amounts between \$ 5,000 and \$ 20,000, with a smaller number of loans having loan amounts below \$ 5,000 or above \$ 20,000. This could potentially indicate that most borrowers on the Lending Club platform are requesting relatively modest loans. This could be due to a variety of factors, such as the borrowers' credit scores, income levels, or financial goals. For example, borrowers with lower credit scores might be more likely to request smaller loans in order to improve their chances of being approved. Similarly, borrowers with lower income levels might be more likely to request smaller loans in order to keep their monthly payments more manageable. On the other hand, borrowers with higher credit scores or higher income levels might be more likely to request larger loans in order to finance more expensive purchases, such as home renovations or a new car. Overall, this pattern could potentially indicate that most borrowers on the Lending Club platform are using the platform to finance relatively small expenses or purchases, rather than using it to borrow large sums of money.

"""

plt.figure(figsize = [12, 5])
df['loan_amount'].plot(kind='hist', bins=20)
plt.title('Distribution of Loan Amounts')
plt.xlabel('Loan Amount')
plt.ylabel('Frequency')
plt.show()

"""* `Histogram of the distribution of interest rate`

This histogram shows the frequency of different values of the interest rate. The majority of loans have interest rates between 5% and 15%, with a small number of loans having interest rates above 15%. This could potentially indicate that most borrowers on the Lending Club platform are being offered relatively moderate interest rates. This could be due to a variety of factors, such as the borrowers' credit scores, loan terms, or the overall lending environment and that most borrowers on the Lending Club platform are being offered interest rates that are within a relatively narrow range, which might be due to the lender's risk-based pricing strategy.
"""

plt.figure(figsize = [12, 5])
df['interest_rate'].plot(kind='hist', bins=20)
plt.title('Interest Rate Distribution (%)')
plt.xlabel('Interest Rate Percentage (%)')
plt.ylabel('Count')
plt.show()

"""* `Boxplot of the distribution of interest rate by loan status`

The boxplot shows the distribution of interest rates for loans in different loan statuses. The boxplot uses the median, first and third quartiles, and minimum and maximum values to summarize the distribution of interest rates for each loan status. We can see that the distribution of interest rates is generally higher for loans that are in the grace period or late compared to loans that are current, fully paid, or have been charged off. This suggests that borrowers with loans in the grace period or late may be more likely to pay higher interest rates compared to borrowers with loans in other statuses. 
"""

plt.figure(figsize = [12, 5])
sns.boxplot(data=df,y='interest_rate',x='loan_status')
plt.title('Interest Rate & Loan Status')
plt.ylabel('Interest Rate')
plt.xlabel('Loan Status')
plt.xticks(rotation=15)

"""* `Violit plot for the relationship between interest rate and loan purpose`

The violin plot shows the relationship between interest rates and loan purposes. The distribution of interest rates is generally higher for loans with the 'small business' and 'renewable energy' purposes compared to other loan purposes. We can also see that the distribution of interest rates for loans with the 'home improvement', 'major purchase', and 'other' purposes is relatively wide, indicating that there is a higher degree of variation in the interest rates for these loan purposes. This suggests that there may be other factors that are influencing the interest rates of loans with these purposes, such as the borrower's creditworthiness or the loan terms.
"""

plt.figure(figsize = [18, 5])
sns.violinplot(data=df,x='loan_purpose', y='interest_rate')
plt.title('Interest Rate for different Loan Purposes')
plt.xlabel('Loan Purpose ')
plt.ylabel('Interest Rate')

"""* `Pie Chart of the proportion of loans by loan status`.

The pie chart shows the proportion of loans by loan status. The majority of loans are Current loans, followed by Fully Paid loans and this could suggest that most borrowers are making timely payments and are on track to fully repay their loans. The small proportion of Grace Period loans could further support this idea, as it suggests that only a small percentage of borrowers are having difficulty making their payments. However, the presence of Late loans and Charged Off loans could indicate that a small percentage of borrowers are experiencing more serious payment difficulties, and may be at risk of default or having their loans declared as a loss. Overall, this pattern could potentially indicate that the Lending Club platform is generally successful in facilitating timely repayment of loans by its borrowers, but that a small percentage of borrowers are experiencing payment difficulties.


"""

plt.figure(figsize = [18, 10])
df['loan_status'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('Proportion of Loans by Loan Status')
plt.legend(title='Loan Status')
plt.show()

"""* `Countplot of Homeownership Values`

The countplot shows the frequency of loans by homeownership status for the loans, with the majority of borrowers in the Mortgage category, followed by Rent. This could suggest that most borrowers on the Lending Club platform are homeowners who are paying off a mortgage and this could be due to the fact that homeowners generally have higher credit scores and are perceived as lower risk by lenders, which could make them more likely to be approved for loans. The relatively small proportion of borrowers in the Own category could further support this idea, as it suggests that only a small percentage of borrowers own their homes outright. Overall, this pattern could potentially indicate that most borrowers on the Lending Club platform are homeowners who are paying off a mortgage, while a smaller proportion are renters.
"""

plt.figure(figsize=(20, 7))
homeownership = df.homeownership.unique().tolist()
plt.subplot(2, 2, 1)
sns.countplot(y='homeownership', data=df)
plt.grid(False)
plt.ylabel("Homeownership")
plt.xlabel("Count")
plt.show()

"""* `Scatterplot for the relationship between loan amount and loan purposes.`

The scatterplot shows the relationship between loan amounts and loan purposes. We can see that there are clusters of loans with different loan purposes, and that the loan amounts tend to vary based on the loan purpose. For example, loans with the 'home improvement' and 'major purchase' purposes tend to have higher loan amounts compared to loans with other purposes. We can also see that there is a lot of overlap in the loan amounts for different loan purposes, which suggests that the loan amount may not be the only factor influencing the loan purpose. Other factors, such as the borrower's creditworthiness or the loan terms, may also be important in determining the loan purpose.
"""

plt.figure(figsize = [15, 5])
plt.scatter(data = df, x = 'loan_amount', y = 'loan_purpose',  alpha =  0.1)
plt.title('Scatter plot for Loan Amount by Purpose')
plt.xlabel('Loan Amount')
plt.ylabel('Loan Purpose')
plt.show()

"""# **Part B: Model Training and Predictions**

Machine Learning algorithms cannot handle categorical values directly, as they expect input data to be numeric. Therefore, we have to encode the categorical values in the dataset as numeric before applying a Machine Learning algorithm. In this way, we can include the categorical data in the models and make use of the information contained in the categorical values to make predictions or classify data. \\

We will use Ordinal Encoding. It is important to note that this encoder does not create a new column for each category. This means that it does not create new features for each category.
"""

df.info()

"""## Encoding"""

cat_list = []
for col in df.columns:
  if df[col].dtypes == 'object':
    cat_list.append(col)
cat_list

"""* `issue_month` : We wouldn't know before whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_month. This would cause **Data Leakage**. So we are going to drop this column."""

df.drop(columns=['issue_month'], inplace=True)

"""* `grade` and `sub_grade`:

`grade` is just a sub-feature of `sub_grade`. So we are goinig to drop its column.
"""

df.drop(columns=['grade'], inplace=True)

"""* For the rest categorical variable, we will apply an encoding method."""

cat_list_enc = ['state', 'homeownership', 'verified_income', 'loan_purpose',
                'application_type', 'loan_status', 'initial_listing_status',
                'disbursement_method', 'sub_grade']
encoder = OrdinalEncoder()
df[cat_list_enc] = df[cat_list_enc].astype(str)
encoder.fit(df[cat_list_enc])
df[cat_list_enc] = encoder.transform(df[cat_list_enc])

"""#### Check for duplicates columns and features

"""

print(f"Dataset size: {df.shape}")

# Remove Duplicate Features
df = df.T.drop_duplicates()
df = df.T

# Remove Duplicate Rows
df.drop_duplicates(inplace=True)

print(f"Dataset size: {df.shape}")

df.info()

"""## Correlation with the Target Value

After encoding the categorical values, we will check the correlation between the variables and the target value of the dataset. It is an important step in the data exploration and preprocessing process.
"""

df.corr().abs()['interest_rate'].sort_values(ascending=False)

"""In general, having variables that are high correlated with the target variable can be useful for building a predictive model, as they can provide strong signals about the target variable. However, it is important to consider the potential drawbacks of using such variables, as they can also introduce problems such as multicollinearity and overfitting.

Multicollinearity refers to the situation where two or more independent variables are highly correlated with each other. This can cause problems in regression models, as it can make it difficult to accurately estimate the coefficients of the independent variables.

Overfitting refers to the situation where a model performs well on the training data, but poorly on unseen data. This can happen when a model is too complex or has too many parameters, and it can lead to poor generalization performance.

We can see that `sub_grade` has **99% correlation** with the target variable `interest_rate`. Drop this column. 
"""

df.drop(columns=['sub_grade'], inplace=True)

"""So for the training, we have **44** features."""

df.shape

"""## Baseline Training

* **Mean Absolute Error** (MAE) and **Mean Squared Error** (MSE) are measures of the difference between the predicted values and the true values. The MAE os calculated by taking the absolute difference between the predicted and true values for each data point, and then averaging these absolute difference, while the MSE is calculated by taking the **squared difference** between the predicted and true values for each data point, and then averaging these squared differences. **MAE** is a **simple** and **intuitive** metric that is easy to understand and interpret, as it represents the average error in the model's predictions in the same units as the data and can be useful for identifying the overall accuracy of a model, as well as for identifying any systematic bias in the model's predictions. **MSE** is a **more sensitive measure** of error than the mean absolute error (MAE), as it punishes larger errors more heavily. **This can make the MSE a better choice for datasets with large errors or outliers**, as it can help to identify these issues more clearly. **However**, it is important to note that the MSE is in squared units, which can make it more difficult to interpret than the MAE, which is in the same units as the data.

* **R2 Score**, also known as the **coefficient of determination**, is a measure of how well the model fits the data. It is calculated by taking the squared difference between the predicted values and the true values, and dividing this by the total variance in the true values. The resulting score is between 0 and 1, with a score of 1 indicating a perfect fit and a score of 0 indicating a poor fit. The R2 Score is a commonly used metric for evaluating the performance of a Regression Model. It can be useful for identifying the **overall accuracy** of a model, as well as for **comparing** the performance of different models. 

* **Train Score** is the model's performance on the **training data**, which is the data used to fit the model. It represents the model's ability to fit the data well and make accurate predictions. On the other hand, **Test Score** is the model's performance on the **test data**, which is a separate dataset that is used to evaluate the model's performance. It represents the model's ability to generalize to **new** data and make accurate predictions.

* **Cross-Validation** is a technique used to evaluate a model's performance by splitting the data into multiple folds, training the model on some of the folds, and evaluating it on the remaining folds. The **Cross-Validation Score** is the average performance of the model across all of the folds. It is a more robust measure of the model's performance than the train score or test score, as it takes into account the model's performance on multiple different splits of the data.

* **Accuracy** of a model is calculated by subtracting the mean value of MAPE from 100. This means that a higher accuracy score corresponds to a lower mean MAPE, and therefore a smaller average error in the model's predictions. Overall, this formula is used to calculate the accuracy of a regression model in terms of the average error of its predictions, with a higher accuracy score indicating a lower average error.
"""

def training(MODEL, X, y, SCALER):
  
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  
  if SCALER == 'True':
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)

  model = MODEL
  start_time = datetime.datetime.now()
  model.fit(X_train, y_train)
  end_time = datetime.datetime.now()
  elapsed_time = end_time - start_time
  print('Training time:', str(elapsed_time))
  predict = model.predict(X_test)
  print("\nMean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')

  start_time = datetime.datetime.now()
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=5)))
  end_time = datetime.datetime.now()
  elapsed_time = end_time - start_time
  print('\nCross Validation time:', str(elapsed_time))

  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  
  return

"""Models used: 

* `LinearRegression()` is based on the assumption that the relationship between the predictor variables and the outcome variable is **linear**, meaning that the change in the outcome variable is **directly proportional** to the change in the predictor variables. It is a **widely used** and **well-understood** model that is simple to implement and easy to interpret. It can be used for a variety of applications, including predicting future values, identifying trends and patterns in data, and understanding the underlying relationships between variables.
* `Lasso()` is a type of Regularized Linear Regression that is used to **prevent overfitting** and **improve the interpretability** of the model. It does this by adding a **penalty term** to the objective function that is being optimized, which helps to reduce the complexity of the model and shrink the coefficients of the less important features to zero. The Lasso model is named after the **L1 Regularization term** that is used to impose the penalty on the model's coefficients. It is a popular choice for **feature selection**, as it can automatically select the most important features for the model and eliminate the less important ones.
* `RandomForestRegressor()` is an **ensemble** model, which means that it is made up of **multiple decision trees** that are trained to make predictions independently and then combined to make a final prediction. RandomForestRegressor is a **powerful** and **flexible** model that is able to **handle complex relationships** between variables and can handle large amounts of data. It is also **resistant to overfitting**, as the use of multiple decision trees helps to average out the errors made by individual trees.
* `DecisionTreeRegressor()` works by creating a **tree-like** structure in which each internal node represents a decision based on the value of one of the predictor variables, and each leaf node represents a prediction. DecisionTreeRegressor is a **simple** and **interpretable** model that is easy to implement and understand. It is particularly useful for understanding the underlying relationships between the predictor variables and the outcome variable, as the tree structure provides a clear visual representation of these relationships.
* `SVR()` works by finding the **hyperplane** in a **high-dimensional space** that maximally separates the data points based on their values for the outcome variable. SVR is a **powerful** and **flexible** model that is able to handle **complex relationships** between variables and can handle large amounts of data. It is also **resistant to overfitting**, as it seeks to find the hyperplane that maximally separates the data points rather than fitting the data as closely as possible.
"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

baseline_models = [LinearRegression(), Lasso(), RandomForestRegressor(), DecisionTreeRegressor(), SVR()]
for model in baseline_models: 
  print('Model: ', str(model))
  training(model, X, y, SCALER=False)
  print('\n################################\n')

"""* `Results from Baseline Training`:

| Regressor         | Accuracy | MAE |
|-------------------|----------|-----|
| Linear Regression | 80.769 % | 2.134 |
| Lasso Regression  | 78.919 % | 2.3 |
| Random Forest Regression | 93.911 % | 0.705 | 
| Decision Tree Regression | 90.824 % | 1.095 |
| Support Vector Machine Regressor | 68.719 % | 3.623 |
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy = [80.769, 78.919, 93.906, 90.724, 68.719]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))
plt.bar(X_axis, accuracy, 0.5)
plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
mae = [2.134, 2.3, 0.705, 1.091, 3.623 ]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))
plt.bar(X_axis, mae, 0.5)
plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("MAE")
plt.legend()
plt.show()

"""The above plots indicate that the `RandomForestRegressor()` and `DecisionTreeRegressor()` models are performing the best out of the five models, with high accuracy scores on the training data. The other models, including `LinearRegression()`, `Lasso()`, and `SVR()`, have lower accuracy scores on the training data. This could potentially suggest that the `RandomForestRegressor()` and `DecisionTreeRegressor()` models are **better suited** for the task at hand and may be more likely to make accurate predictions on new data.

## Hyperparameter Tuning

* `Check for Scalling Method` :
"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

baseline_models = [LinearRegression(), Lasso(), RandomForestRegressor(), DecisionTreeRegressor(), SVR()]
for model in baseline_models: 
  print('Model: ', str(model))
  training(model, X, y, SCALER=True)
  print('\n################################\n')

"""* `Results after Scalling` : 

| Regressor | Accuracy | MAE | 
|-----------|----------|-----| 
| Linear Regression | 80.769 % | 2.134 |
| Lasso Regression  | 78.919 % | 2.3 |
| Random Forest Regression | 93.848 % | 0.709 |
| Decision Tree Regression | 91.026 % | 1.066 |
| Support Vector Machine Regressor | 68.719 % | 3.623 |
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy_before = [80.769, 78.919, 93.906, 90.724, 68.719]
accuracy_after = [80.769, 78.919, 93.745, 90.778, 68.719]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))

plt.bar(X_axis, accuracy_before, 0.3, label = "Accuracy before Scaling")
plt.bar(X_axis + 0.3, accuracy_after, 0.3, label = "Accuracy after Scaling")

plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

"""Accuracies are the same, so there is no need for Scaling.

* Tuning hyperparameters of `LinearRegression()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = LinearRegression()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')

  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component

print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=40)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=40)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {'fit_intercept': [True, False]}
model = LinearRegression()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=4)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`fit_intercept = True` is the default value, so we keep this model as it is. """

model = LinearRegression()
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=5)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 80.769 %` \\

`Accuracy now: 80.783 %`

* Tuning hyperparameters of `Lasso()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = Lasso()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=35)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=35)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {'alpha': [0.01, 0.1, 0.5, 1, 5, 10],
              'fit_intercept': [True, False],
              'max_iter': [2, 5, 10, 100],
              'tol': [1e-4, 1e-6, 1e-8],
              'normalize': [True, False]}

model = Lasso()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`alpha = 0.01`, `fit_intercept = True`, `max_iter = 100`, `normalize = False`, `tol = 1e-08`"""

model = Lasso(alpha=0.01, fit_intercept=True, max_iter=100, normalize=False, tol=1e-08)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 78.919 %` \\

`Accuracy now: 80.518 %`

* Tuning hyperparameters of `RandomForestRegressor()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = RandomForestRegressor()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=10)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=10)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'n_estimators': [20, 50, 80, 100], 
    'max_depth': [5, 10, 15, 20, 25], 
    'min_samples_split': [1, 2, 5], 
    'min_samples_leaf': [1, 2, 5],
    'bootstrap': [True, False]
}

model = RandomForestRegressor()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)

grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""When trying to perform `GridSearchCV()` with the different hyperparameters, the training process took a lot of time to search all the possible combinations. For this reason, I chose to perform `GridSearchCV()` for every parameter separately and then combine the best from each one.

`bootstrap=True`, `max_depth=20`, `min_samples_leaf=2`, `min_samples_split=5`, `n_estimators=100`
"""

model = RandomForestRegressor(bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 93.9119 %` \\

`Accuracy now: 89.692 %`

* Tuning hyperparameters of `DecisionTreeRegressor()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = DecisionTreeRegressor()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=10)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=10)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'max_depth': [1, 2, 5, 10, 12],
    'max_leaf_nodes': [5, 10, 20, 40],
    'min_samples_leaf': [1, 3, 5, 8, 10, 12],
    'min_weight_fraction_leaf': [0.01, 0.05, 0.1, 0.2, 0.5],
    'splitter':["best", "random"],
}

model = DecisionTreeRegressor()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`max_depth=10`, `max_leaf_nodes=40`, `min_samples_leaf=1`, `min_weight_fraction_leaf=0.01`, `splitter='best'`"""

model = DecisionTreeRegressor(max_depth=10, max_leaf_nodes=40, min_samples_leaf=1, min_weight_fraction_leaf=0.011, splitter='best')
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 90.824  %` \\

`Accuracy now: 80.54 %`

* Tuning hyperparameters of `SVR()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = SVR()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""PCA did not help that much, but we will use `PCA(n_components=38)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=38)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'C': [0.1, 1, 10, 50, 100],
    'epsilon': [0.1, 0.3, 0.5, 0.7, 0.9], 
    'degree': [2,3,4]
}

model = SVR()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""As in `RandomForestRegressor()` hyperparameter tuning, when trying to perform `GridSearchCV()` with the different hyperparameters, the training process took a lot of time to search all the possible combinations. For this reason, I chose to perform `GridSearchCV()` for every parameter separately and then combine the best from each one.

`C=100`, `epsilon=0.7`, `kernel=000`, `degree=000`
"""

model = SVR(C=100, epsilon=0.7, degree=2)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 68.719  %` \\

`Accuracy now: 73.878 %`

After tuning the hyperparameters of each one of the above-mentioned models, we came up with the following results: 

| Regressor | Accuracy | MAE | 
|-----------|----------|-----| 
| Linear Regression | 80.783 % | 2.133 |
| Lasso Regression  | 80.518 % | 2.15 |
| Random Forest Regression | 89.692 % | 1.154 |
| Decision Tree Regression | 80.54 % | 2.16 |
| Support Vector Machine Regressor | 73.878 % | 3.02 |
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy_before = [80.769, 78.919, 93.848, 91.026, 68.719]
accuracy_after = [80.783, 80.518, 89.692, 80.54, 73.878]

fig = plt.figure(figsize = (15, 6))  
X_axis = np.arange(len(regressors))

plt.bar(X_axis, accuracy_before, 0.3, label = "Accuracy Baseline Models")
plt.bar(X_axis + 0.3, accuracy_after, 0.3, label = "Accuracy after Hyper-parameters' Tuning")

plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

"""As wee see from the plot, after hyper-parameters' Tuning some of the above models have better and some worse accuracy than their baseline version. 

* `LinearRegression()` has almost the same accuracy. 
* `Lasso()` and `SVR()` have slightly better accuracy with the tuning of the possible hyper-parameters.
* `RandomForestRegressor()` and `DecisionTreeRegressor()` had **better performance** on its first baseline version. 


\\
Due to **lack of time**, the `GridSearchCV()` of the hyper-parameters was limited to a small range of possible values. With much more time given, it could be possible to try different values in order to find the best combination of hyper-parameters for each model. 


If I had more time, I would consider implementing some enhacements to the model in order to improve its performance. More specifically:
* **Feature Engineering**. I would try to use different combinations of features or create new features based on the existing ones. 
* **More Hyperparameter Tuning**. I would try more possible values and combinations. 
* **More Algorithms**. I would try to use different algorithms and models in order to see if they perform better on the data. 
"""