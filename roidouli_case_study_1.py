# -*- coding: utf-8 -*-
"""Roidouli_Case_study_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hKEnPiTCeIDEZOxzE_Ek-3Kmrx1uwVdA

# **Dataset Loading and Libraries Importing**
"""

# Libraries Importing
import pandas as pd

from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import datetime

from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.decomposition import PCA
from sklearn.feature_selection import RFE, mutual_info_regression

from keras.callbacks import EarlyStopping

import warnings
warnings.filterwarnings("ignore")

# Display all columns and rows of the dataset
pd.options.display.max_columns = None
pd.options.display.max_rows = None

drive.mount('/content/gdrive')

# Load dataset
df = pd.read_csv("/content/gdrive/MyDrive/stout/loans_full_schema.csv")

"""# **Part A: Analysis of the Dataset and Visualizations**

## **Quick Look at the Dataset**

The following dataset has **10.000 observiations** with **55 variables** and it represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals.
"""

# Size of dataset
df.shape

# Show 5 random rows of dataset
df.sample(5)

# Quick look in the columns - empty values - datatypes
df.info()

"""## **Analysis of Categorical and Numerical Variables**

#### **A. Categorical Variables**

We have 13 **categorical** variables: 
* `emp_title`: job title of the applicant. (manager, owner, teacher etc)
* `state`: two-letter state code. (CA, TX, NY etc)
* `homeownership`: the ownership status of the applicant's residence. (MORTGAGE, RENT, OWN)
* `verified_income`: type of verification of the applicant's income. (Source Verified, Not Verified, Verified)
* `verification_income_joint`: type of verification of the joint income (Source Verified, Not Verified, Verified)
* `loan_purpose`: category for the purpose of the loan. (debt consolidation, credit card, home improvement etc)
* `application_type`: type of application (individual, joint)
* `grade`: grade associated with the loan. The grade reflects the creditworthiness and the risk associated with the loan. In grading system "A-D", "A" referes to the highest quality and "D" to the lowest. (A, B, C, D, E, F, G)
* `sub_grade`: detailed grade associated with the loan (A1, A2, B1, B2, etc)
* `issue_month`: month the loan was issued (Jan-2018, Feb-2018, Mar-2018)
* `loan_status`: status of the loan (Current, Fully Paid, In Grace Period, Late (31-120 days), Late (16-30 days))
* `initial_listing_status`: initial listing status of the loan (whole, fractional)
* `disbursement_method`: dispersement method of the loan. (Cash, DirectPay)
"""

# Categorical values
df.describe(include='object')

"""#### **B. Numerical Variables**

We have 42 **numerical** variables: 
* `emp_length`: Number of years in the job, rounded down. 
* `annual_income`: The annual income of the applicant.
* `debt_to_income`: Debt-to-income ratio.  
* `annual_income_joint`: if this is a joint application, then the annual income of the two parties applying.
* `debt_to_income_joint`: Debt-to-income ratio for the two parties.
* `delinq_2y`: Delinquencies on lines of credit in the last 2 years. 
* `months_since_last_delinq`: Months since the last delinquency.
* `earliest_credit_line`: Year of the applicant's earliest line of credit. 
* `inquiries_last_12m`: Inquiries into the applicant's credit during the last 12 months. 
* `total_credit_lines`: Total number of credit lines in this applicant's credit history. 
* `open_credit_lines`: Number of currently open lines of credit. 
* `total_credit_limit`: Total available credit, e.g. if only credit cards, then the total of all the credit limits. This excludes a mortgage.
* `total_credit_utilized`: Total credit balance, excluding a mortgage.
* `num_collections_last_12m`: Number of collections in the last 12 months. This excludes medical collections. 
* `num_historical_failed_to_pay`: The number of derogatory public records, which roughly means the number of times the applicant failed to pay.
* `months_since_90d_late`: Months since the last time the applicant was 90 days late on a payment.
* `current_accounts_delinq`: Number of accounts where the applicant is currently delinquent. 
* `total_collection_amount_ever`: The total amount that the applicant has had against them in collections. 
* `current_installment_accounts`: Number of installment accounts, which are (roughly) accounts with a fixed payment amount and period. A typical example might be a 36-month car loan. 
* `accounts_opened_24m`: Number of new lines of credit opened in the last 24 months
* `months_since_last_credit_inquiry`:  Number of months since the last credit inquiry on this applicant. 
* `num_satisfactory_accounts`: Number of satisfactory accounts. 
* `num_accounts_120d_past_due`: Number of current accounts that are 120 days past due. 
* `num_accounts_30d_past_due`: Number of current accounts that are 30 days past due. 
* `num_active_debit_accounts`: Number of currently active bank cards. 
* `total_debit_limit`: Total of all bank card limits.
* `num_total_cc_accounts`: Total number of credit card accounts in the applicant's history. 
* `num_open_cc_accounts`: Total number of currently open credit card accounts.
* `num_cc_carrying_balance`: Number of credit cards that are carrying a balance.
* `num_mort_accounts`: Number of mortgage accounts.
* `account_never_delinq_percent`: Percent of all lines of credit where the applicant was never delinquent.
* `tax_liens`: a numeric vector 
* `public_record_bankrupt`: Number of bankruptcies listed in the public record for this applicant. 
* `loan_amount`: The amount of the loan the applicant received. 
* `term`: The number of months of the loan the applicant received.
* `interest_rate`: Interest rate of the loan the applicant received.
* `installment`: Monthly payment for the loan the applicant received. 
* `balance`: Current balance on the loan. 
* `paid_total`: Total that has been paid on the loan by the applicant.
* `paid_principal`: The difference between the original loan amount and the current balance on the loan.
* `paid_interest`: The amount of interest paid so far by the applicant.
* `paid_late_fees`: Late fees paid by the applicant.
"""

# Numerical values
df.describe()

"""## **Null Values Handling**"""

# Check if there are any empty cells in the dataset
df.isnull().values.any()

def plot_null_values(df):
  if df.isnull().sum().sum() != 0:
    na_df = (df.isnull().sum() / len(df)) * 100
    na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)
    missing_data = pd.DataFrame({'Missing Ratio %' :na_df})
    missing_data.plot(kind = "barh")
    plt.show()
  else:
    print('No NAs found')
  return

plot_null_values(df)

"""Null values add bias into the analysis if they are not handled properly and also can affect the performance of a machine learning model. Many machine learning algorithms cannot handle null values and will fail or produce inaccurate results. Removing null values can help ensure that the machine learning model is trained on a clean, consistent dataset. """

for column in df.columns:
  if df[column].isna().sum() != 0 :
    missing = df[column].isna().sum()
    portion = (missing / df.shape[0]) * 100
    print(f"'{column}': number of missing values '{missing}' ==> '{portion:.3f}%'")

"""* `emp_title` : """

df.emp_title.nunique()

"""Realistically there are **too many unique** job titles to try to convert this to a dummy variable feature. We will remove the `emp_title` column."""

df.drop(columns=['emp_title'], inplace=True)

"""* `emp_length` : """

df.emp_length.unique()

for year in df.emp_length.unique():
    print(f"{year} years in this position:")
    print(f"{df[df.emp_length == year].loan_status.value_counts(normalize=True)}")
    print('==========================================')

"""Rates are extremely similar across all employment lengths. So we are going to drop the `emp_length` column."""

df.drop(columns=['emp_length'], inplace=True)

"""* `debt_to_income` :"""

df.debt_to_income.value_counts().head()

"""The null value ration of `debt_to_income` is very small, so we replace the null values with the mean value of this column."""

df['debt_to_income'].fillna(df['debt_to_income'].mean(), inplace=True)

"""* `annual_income_joint` , `debt_to_income_joint`, `verification_income_joint`

The fact that there are too many null values refer to no joint. We are going to drop these columns.
"""

df.drop(columns=['annual_income_joint', 'debt_to_income_joint', 'verification_income_joint'], inplace=True)

"""* `months_since_last_delinq` : """

df.months_since_last_delinq.value_counts().head()

"""The null value ratio of `months_since_last_delinq` is somehow large. The null values refer to the fact the there was never an delinquency. So we could set a very big number or a number bigger than the maximum value of this column, so as to not confuse the model by setting the zero value. """

df['months_since_last_delinq'].fillna(df['months_since_last_delinq'].max() + 24, inplace=True)

"""* `months_since_90d_late` :"""

df.months_since_90d_late.value_counts().head(10)

"""There are too many null values, so we are going to drop the `months_since_90d_late` column."""

df.drop(columns=['months_since_90d_late'], inplace=True)

"""* `months_since_last_credit_inquiry` :"""

df.months_since_last_credit_inquiry.value_counts().head(10)

"""The null value ratio of `months_since_last_credit_inquiry` is **not large**. It wouldn't be a good idea to drop the column. The null values refer to the fact the there was never an inquiry. So we could set a very big number or a number bigger than the maximum value of this column, so as to not confuse the model by setting the zero value. """

df['months_since_last_credit_inquiry'].fillna(df['months_since_last_credit_inquiry'].max() + 24, inplace=True)

"""* `num_accounts_120d_past_due` : """

df.num_accounts_120d_past_due.value_counts().head()

"""All values are zero values, so we will drop the `num_accounts_120d_past_due` column."""

df.drop(columns=['num_accounts_120d_past_due'], inplace=True)

"""Let's check for null values again"""

df.isnull().values.any()

"""## **Visualizations**

The Visualizations can give us an understanding for which variables are important.

* `Histogram of the distribution of loan amounts`

This visualization shows the frequency of different loan amounts. The histogram uses a set of bins to group the loan amounts into intervals, and the height of each bar represents the frequency of loans with loan amounts within that interval. We can see that the majority of loans have loan amounts between \$ 5,000 and \$ 25,000, with a smaller number of loans having loan amounts below \$ 5,000 or above \$ 25,000.
"""

plt.figure(figsize = [12, 5])
df['loan_amount'].plot(kind='hist', bins=20)
plt.title('Distribution of Loan Amounts')
plt.xlabel('Loan Amount')
plt.ylabel('Frequency')
plt.show()

"""* `Histogram of the distribution of interest rate`

This visualization shows the frequency of different values of the interest rate. The histogram uses a set of bins to group the interest rates into intervals, and the height of each bar represents the frequency of loans with interest rates within that interval. We can see that the majority of loans have interest rates between 5% and 15%, with a small number of loans having interest rates above 15%. The shape of the distribution suggests that the interest rates are skewed to the left, with a longer tail of higher interest rates. This may indicate that a relatively small number of loans have significantly higher interest rates compared to the rest of the loans in the dataset.
"""

plt.figure(figsize = [12, 5])
df['interest_rate'].plot(kind='hist', bins=20)
plt.title('Interest Rate Distribution (%)')
plt.xlabel('Interest Rate Percentage (%)')
plt.ylabel('Count')
plt.show()

"""* `Boxplot of the distribution of interest rate by loan status`

The boxplot shows the distribution of interest rates for loans in different loan statuses. The boxplot uses the median, first and third quartiles, and minimum and maximum values to summarize the distribution of interest rates for each loan status. We can see that the distribution of interest rates is generally higher for loans that are in the grace period or late compared to loans that are current, fully paid, or have been charged off. This suggests that borrowers with loans in the grace period or late may be more likely to pay higher interest rates compared to borrowers with loans in other statuses. 
"""

plt.figure(figsize = [12, 5])
sns.boxplot(data=df,y='interest_rate',x='loan_status')
plt.title('Interest Rate & Loan Status')
plt.ylabel('Interest Rate')
plt.xlabel('Loan Status')
plt.xticks(rotation=15)

"""* `Violit plot for the relationship between interest rate and loan purpose`

The violin plot shows the relationship between interest rates and loan purposes. The plot uses a kernel density estimate to show the distribution of interest rates for each loan purpose. We can see that the distribution of interest rates is generally higher for loans with the 'small business' and'renewable energy' purposes compared to other loan purposes. We can also see that the distribution of interest rates for loans with the 'home improvement', 'major purchase', and 'other' purposes is relatively wide, indicating that there is a higher degree of variation in the interest rates for these loan purposes. This suggests that there may be other factors that are influencing the interest rates of loans with these purposes, such as the borrower's creditworthiness or the loan terms.
"""

plt.figure(figsize = [18, 5])
sns.violinplot(data=df,x='loan_purpose', y='interest_rate')
plt.title('Interest Rate for different Loan Purposes')
plt.xlabel('Loan Purpose ')
plt.ylabel('Interest Rate')

"""* `Pie Chart of the proportion of loans by loan status`.

The pie chart shows the proportion of loans by loan status. We can see that the majority of loans are Current loans, followed by Fully Paid loans, and then in Grace Period loans. The pie chart reveals that only a small percentage of loans have been charged off, which suggests that the overall performance of the loans in the dataset is relatively strong. It also shows that the proportion of current loans is relatively high, which may indicate that borrowers are making timely payments on their loans.
"""

plt.figure(figsize = [18, 10])
df['loan_status'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('Proportion of Loans by Loan Status')
plt.legend(title='Loan Status')
plt.show()

"""* `Countplot of Homeownership Values`

The countplot shows the frequency of loans by homeownership status for the loans. We can see that the majority of borrowers are mortgagees, followed by renters and finally homeowners. The countplot reveals that a relatively small percentage of borrowers are homeowners, which may suggest that homeownership is an important factor for borrowers in this dataset. It also shows that the proportion of renters is relatively high, which may indicate that there is a significant number of borrowers who do not own their own homes.
"""

plt.figure(figsize=(20, 7))
homeownership = df.homeownership.unique().tolist()
plt.subplot(2, 2, 1)
sns.countplot(y='homeownership', data=df)
plt.grid(False)
plt.ylabel("Homeownership")
plt.xlabel("Count")
plt.show()

"""* `Scatterplot for the relationship between loan amount and loan purposes.`

The scatterplot shows the relationship between loan amounts and loan purposes. We can see that there are clusters of loans with different loan purposes, and that the loan amounts tend to vary based on the loan purpose. For example, loans with the 'home improvement' and 'major purchase' purposes tend to have higher loan amounts compared to loans with other purposes. We can also see that there is a lot of overlap in the loan amounts for different loan purposes, which suggests that the loan amount may not be the only factor influencing the loan purpose. Other factors, such as the borrower's creditworthiness or the loan terms, may also be important in determining the loan purpose.
"""

plt.figure(figsize = [15, 5])
plt.scatter(data = df, x = 'loan_amount', y = 'loan_purpose',  alpha =  0.1)
plt.title('Scatter plot for Loan Amount by Purpose')
plt.xlabel('Loan Amount')
plt.ylabel('Loan Purpose')
plt.show()

"""# **Part B: Model Training and Predictions**

Machine Learning algorithms cannot handle categorical values directly, as they expect input data to be numeric. Therefore, we have to encode the categorical values in the dataset as numeric before applying a Machine Learning algorithm. In this way, we can include the categorical data in the models and make use of the information contained in the categorical values to make predictions or classify data. \\

We will use Ordinal Encoding. It is important to note that this encoder does not create a new column for each category. This means that it does not create new features for each category.
"""

df.info()

"""## Encoding"""

cat_list = []
for col in df.columns:
  if df[col].dtypes == 'object':
    cat_list.append(col)
cat_list

"""* `issue_month` : We wouldn't know before whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_month. This would cause **Data Leakage**. So we are going to drop this column."""

df.drop(columns=['issue_month'], inplace=True)

"""* `grade` and `sub_grade`:

`grade` is just a sub-feature of `sub_grade`. So we are goinig to drop its column.
"""

df.drop(columns=['grade'], inplace=True)

"""* For the rest categorical variable, we will apply an encoding method."""

cat_list_enc = ['state', 'homeownership', 'verified_income', 'loan_purpose',
                'application_type', 'loan_status', 'initial_listing_status',
                'disbursement_method', 'sub_grade']
encoder = OrdinalEncoder()
df[cat_list_enc] = df[cat_list_enc].astype(str)
encoder.fit(df[cat_list_enc])
df[cat_list_enc] = encoder.transform(df[cat_list_enc])

"""#### Check for duplicates columns and features

"""

print(f"Dataset size: {df.shape}")

# Remove Duplicate Features
df = df.T.drop_duplicates()
df = df.T

# Remove Duplicate Rows
df.drop_duplicates(inplace=True)

print(f"Dataset size: {df.shape}")

df.info()

"""## Correlation with the Target Value

After encoding the categorical values, we will check the correlation between the variables and the target value of the dataset. It is an important step in the data exploration and preprocessing process.
"""

df.corr().abs()['interest_rate'].sort_values(ascending=False)

"""In general, having variables that are high correlated with the target variable can be useful for building a predictive model, as they can provide strong signals about the target variable. However, it is important to consider the potential drawbacks of using such variables, as they can also introduce problems such as multicollinearity and overfitting.

Multicollinearity refers to the situation where two or more independent variables are highly correlated with each other. This can cause problems in regression models, as it can make it difficult to accurately estimate the coefficients of the independent variables.

Overfitting refers to the situation where a model performs well on the training data, but poorly on unseen data. This can happen when a model is too complex or has too many parameters, and it can lead to poor generalization performance.

We can see that `sub_grade` has **99% correlation** with the target variable `interest_rate`. Drop this column. 
"""

df.drop(columns=['sub_grade'], inplace=True)

"""So for the training, we have **44** features."""

df.shape

"""## Baseline Training"""

def training(MODEL, X, y, SCALER):
  
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  
  if SCALER == 'True':
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)

  model = MODEL
  start_time = datetime.datetime.now()
  model.fit(X_train, y_train)
  end_time = datetime.datetime.now()
  elapsed_time = end_time - start_time
  print('Training time:', str(elapsed_time))
  predict = model.predict(X_test)
  print("\nMean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')

  start_time = datetime.datetime.now()
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=5)))
  end_time = datetime.datetime.now()
  elapsed_time = end_time - start_time
  print('\nCross Validation time:', str(elapsed_time))

  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  
  return

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

baseline_models = [LinearRegression(), Lasso(), RandomForestRegressor(), DecisionTreeRegressor(), SVR()]
for model in baseline_models: 
  print('Model: ', str(model))
  training(model, X, y, SCALER=False)
  print('\n################################\n')

"""| Regressor         | Accuracy | MAE |
|-------------------|----------|-----|
| Linear Regression | 80.769 % | 2.134 |
| Lasso Regression  | 78.919 % | 2.3 |
| Random Forest Regression | 93.911 % | 0.705 | 
| Decision Tree Regression | 90.824 % | 1.095 |
| Support Vector Machine Regressor | 68.719 % | 3.623 |

Results:
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy = [80.769, 78.919, 93.906, 90.724, 68.719]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))
plt.bar(X_axis, accuracy, 0.5)
plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
mae = [2.134, 2.3, 0.705, 1.091, 3.623 ]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))
plt.bar(X_axis, mae, 0.5)
plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("MAE")
plt.legend()
plt.show()

"""## Hyperparameter Tuning

* Check for Scalling Method :
"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

baseline_models = [LinearRegression(), Lasso(), RandomForestRegressor(), DecisionTreeRegressor(), SVR()]
for model in baseline_models: 
  print('Model: ', str(model))
  training(model, X, y, SCALER=True)
  print('\n################################\n')

"""After Scalling we have: 

| Regressor | Accuracy | MAE | 
|-----------|----------|-----| 
| Linear Regression | 80.769 % | 2.134 |
| Lasso Regression  | 78.919 % | 2.3 |
| Random Forest Regression | 93.848 % | 0.709 |
| Decision Tree Regression | 91.026 % | 1.066 |
| Support Vector Machine Regressor | 68.719 % | 3.623 |

Results:
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy_before = [80.769, 78.919, 93.906, 90.724, 68.719]
accuracy_after = [80.769, 78.919, 93.745, 90.778, 68.719]

fig = plt.figure(figsize = (10, 4))  
X_axis = np.arange(len(regressors))

plt.bar(X_axis, accuracy_before, 0.3, label = "Accuracy before Scaling")
plt.bar(X_axis + 0.3, accuracy_after, 0.3, label = "Accuracy after Scaling")

plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

"""Accuracies are the same, so it is no need to use Scaling.

* Tuning hyperparameters of `LinearRegression()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = LinearRegression()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')

  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component

print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=40)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=40)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {'fit_intercept': [True, False]}
model = LinearRegression()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=4)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`fit_intercept = True` is the default value, so we keep this model as it is. """

model = LinearRegression()
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=5)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 80.769 %` \\

`Accuracy now: 80.783 %`

* Tuning hyperparameters of `Lasso()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = Lasso()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=35)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=35)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {'alpha': [0.01, 0.1, 0.5, 1, 5, 10],
              'fit_intercept': [True, False],
              'max_iter': [2, 5, 10, 100],
              'tol': [1e-4, 1e-6, 1e-8],
              'normalize': [True, False]}

model = Lasso()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`alpha = 0.01`, `fit_intercept = True`, `max_iter = 100`, `normalize = False`, `tol = 1e-08`"""

model = Lasso(alpha=0.01, fit_intercept=True, max_iter=100, normalize=False, tol=1e-08)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 78.919 %` \\

`Accuracy now: 80.518 %`

* Tuning hyperparameters of `RandomForestRegressor()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = RandomForestRegressor()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=10)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=10)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'n_estimators': [20, 50, 80, 100], 
    'max_depth': [5, 10, 15, 20, 25], 
    'min_samples_split': [1, 2, 5], 
    'min_samples_leaf': [1, 2, 5],
    'bootstrap': [True, False]
}

model = RandomForestRegressor()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)

grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""When trying to perform `GridSearchCV()` with the different hyperparameters, the training process took a lot of time to search all the possible combinations. For this reason, I chose to perform `GridSearchCV()` for every parameter separately and then combine the best from each one.

`bootstrap=True`, `max_depth=20`, `min_samples_leaf=2`, `min_samples_split=5`, `n_estimators=100`
"""

model = RandomForestRegressor(bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 93.9119 %` \\

`Accuracy now: 89.692 %`

* Tuning hyperparameters of `DecisionTreeRegressor()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = DecisionTreeRegressor()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""We will use `PCA(n_components=10)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=10)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'max_depth': [1, 2, 5, 10, 12],
    'max_leaf_nodes': [5, 10, 20, 40],
    'min_samples_leaf': [1, 3, 5, 8, 10, 12],
    'min_weight_fraction_leaf': [0.01, 0.05, 0.1, 0.2, 0.5],
    'splitter':["best", "random"],
}

model = DecisionTreeRegressor()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""`max_depth=10`, `max_leaf_nodes=40`, `min_samples_leaf=1`, `min_weight_fraction_leaf=0.01`, `splitter='best'`"""

model = DecisionTreeRegressor(max_depth=10, max_leaf_nodes=40, min_samples_leaf=1, min_weight_fraction_leaf=0.011, splitter='best')
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 90.824  %` \\

`Accuracy now: 80.54 %`

* Tuning hyperparameters of `SVR()`
"""

n_components = [5, 10, 15, 20, 25, 30, 35, 38, 40, 42, 43]

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

accuracy_calc = 0 
error_calc = 100
for component in n_components:
  print('N_COMPONENTS = ', component)
  
  pca = PCA(n_components=component)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  X_train = pca.fit_transform(X_train)
  X_test = pca.transform(X_test)
  
  model = SVR()
  model.fit(X_train, y_train)
  predict = model.predict(X_test)
  
  mae = mean_absolute_error(y_test, predict)
  print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
  print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
  print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

  print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
  print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
  print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
  errors = abs(predict - y_test)
  mape = 100 * (errors / y_test)
  accuracy = 100 - np.mean(mape)
  print('Accuracy               :', round(accuracy,3), '%.') 
  print('##########################################')
  if accuracy > accuracy_calc and mae < error_calc: 
    accuracy_calc = accuracy
    mae = error_calc
    count = component
  
print('Higher Accuracy & Smaller Error ---> n_components=', count)

"""PCA did not help that much, but we will use `PCA(n_components=38)`"""

eval_df = df.copy()
y = eval_df['interest_rate']
eval_df.drop(columns=['interest_rate'], inplace=True)
X = eval_df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=38)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

param_grid = {
    'C': [0.1, 1, 10, 50, 100],
    'epsilon': [0.1, 0.3, 0.5, 0.7, 0.9], 
    'degree': [2,3,4]
}

model = SVR()
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(best_params)

"""As in `RandomForestRegressor()` hyperparameter tuning, when trying to perform `GridSearchCV()` with the different hyperparameters, the training process took a lot of time to search all the possible combinations. For this reason, I chose to perform `GridSearchCV()` for every parameter separately and then combine the best from each one.

`C=100`, `epsilon=0.7`, `kernel=000`, `degree=000`
"""

model = SVR(C=100, epsilon=0.7, degree=2)
start_time = datetime.datetime.now()
model.fit(X_train, y_train)
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('Training time:', str(elapsed_time))
predict = model.predict(X_test)
print("Mean Absolute Error    :", round(mean_absolute_error(y_test, predict), 3))
print("Mean Squared Error     :", round(mean_squared_error(y_test, predict), 3))
print("(R^2) Score            :", round(r2_score(y_test, predict) * 100, 3), '%.')

print('Train Score            :', round(model.score(X_train, y_train) * 100,3), '%.')
print('Test Score             :', round(model.score(X_test, y_test) * 100, 3), '%.')
start_time = datetime.datetime.now()
print("Cross Val Score        :", round(cross_val_score(model, X, y, cv=3).mean() * 100,3), '%, deviation:', np.std(cross_val_score(model, X, y, cv=3)))
end_time = datetime.datetime.now()
elapsed_time = end_time - start_time
print('\nCross Validation time:', str(elapsed_time))
errors = abs(predict - y_test)
mape = 100 * (errors / y_test)
accuracy = 100 - np.mean(mape)
print('Accuracy               :', round(accuracy,3), '%.')

"""`Accuray before: 68.719  %` \\

`Accuracy now: 73.878 %`

After tuning the hyperparameters of each one of the above-mentioned models, we came up with the following results: 

| Regressor | Accuracy | MAE | 
|-----------|----------|-----| 
| Linear Regression | 80.783 % | 2.133 |
| Lasso Regression  | 80.518 % | 2.15 |
| Random Forest Regression | 89.692 % | 1.154 |
| Decision Tree Regression | 80.54 % | 2.16 |
| Support Vector Machine Regressor | 73.878 % | 3.02 |

| Regressor | Accuracy | MAE | 
|-----------|----------|-----| 
| Linear Regression | 80.769 % | 2.134 |
| Lasso Regression  | 78.919 % | 2.3 |
| Random Forest Regression | 93.848 % | 0.709 |
| Decision Tree Regression | 91.026 % | 1.066 |
| Support Vector Machine Regressor | 68.719 % | 3.623 |
"""

regressors = ['Linear', 'Lasso', 'RandomForest', 'DecisionTree', 'SupportVectorMachine']
accuracy_before = [80.769, 78.919, 93.848, 91.026, 68.719]
accuracy_after = [80.783, 80.518, 89.692, 80.54, 73.878]

fig = plt.figure(figsize = (15, 6))  
X_axis = np.arange(len(regressors))

plt.bar(X_axis, accuracy_before, 0.3, label = "Accuracy Baseline Models")
plt.bar(X_axis + 0.3, accuracy_after, 0.3, label = "Accuracy after Hyper-parameters' Tuning")

plt.xticks(X_axis, regressors)
plt.xlabel("Regressors")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.show()

"""As wee see from the plot, after hyper-parameters' Tuning some of the above models have better and some worse accuracy than their baseline version. 

* `LinearRegression()` has almost the same accuracy. 
* `Lasso()` and `SVR()` have slightly better accuracy with the tuning of the possible hyper-parameters.
* `RandomForestRegressor()` and `DecisionTreeRegressor()` had better performance on its first baseline version. 


\\
Due to **lack of time**, the `GridSearchCV()` of the hyper-parameters was limited to a small range of possible values. With much more time given, it could be possible to try different values in order to find the best combination of hyper-parameters for each model. 
"""